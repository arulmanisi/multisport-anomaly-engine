You are my coding agent.

Feature 1: "Anomaly Feed" (SOC-style feed of top anomalies).

Goal (Backend):
Add backend endpoints to power an anomaly feed UI:
- List anomalies with filters (format, min severity, limit)
- Return enough fields for UI cards (headline, scores, narrative snippet)
- No breaking changes to existing endpoints.

Assumptions:
- We have some dataset available locally OR a demo dataset fallback.
- Existing narrator exists (LLM optional) and can generate narrative when requested.

Tasks:

1) Create a module:
   backend/app/anomaly_feed.py

2) Implement data access (MVP):
- Implement a function load_feed_dataset() that attempts in this order:
    a) data/processed/per_innings_with_ups.csv
    b) data/synthetic_ups_dataset.csv
  If neither exists, build a small in-memory demo DataFrame with 30 anomalies
  across 8â€“10 players (P_DEMO_1..P_DEMO_10), formats (T20/ODI/TEST),
  and reasonable columns:
    player_id, match_format, date, current_runs, baseline_mean_runs, baseline_std_runs,
    ups_score, ups_bucket, ups_anomaly_flag_baseline, model_anomaly_probability, model_anomaly_label,
    venue_flatness (optional), opposition_strength (optional), batting_position (optional)

- Normalize columns:
    - player_name -> player_id if needed
    - runs_scored -> current_runs if needed
    - parse date if present else generate placeholder dates

3) Define severity score:
- severity_score = ups_score
- If model_anomaly_probability exists, compute:
    combined_score = 0.7*ups_score + 0.3*(model_prob*5.0)
- Use combined_score when model_prob present else ups_score.

4) Add endpoint:
   GET /feed/anomalies

Query params:
- format: optional str ("T20","ODI","TEST","ALL") default "ALL"
- min_ups: optional float default 0.0
- min_prob: optional float default 0.0
- limit: int default 25 (cap at 100)
- sort: str default "combined" ("combined" or "ups")

Return JSON:
{
  "items": [
    {
      "event_id": "<stable id>",  # can be f"{player_id}-{date}-{format}"
      "player_id": "...",
      "match_format": "...",
      "date": "...",
      "current_runs": ...,
      "baseline_mean_runs": ...,
      "baseline_std_runs": ...,
      "ups_score": ...,
      "ups_bucket": "...",
      "model_anomaly_probability": ...,
      "model_anomaly_label": ...,
      "combined_score": ...,
      "headline": "...",          # rule-based one-liner
      "key_drivers": [ "...", "...", "..." ]  # rule-based bullets
    }, ...
  ]
}

- Implement headline and key_drivers rule-based in backend (so UI stays thin).
  Headline should be sports-media style.
  Key drivers should include UPS magnitude and baseline comparison; add context bullets if available.

5) Add endpoint:
   GET /feed/anomaly/{event_id}

- Returns full detail (same fields as above) plus narrative generated by narrator in requested tone.

Query param:
- tone: optional str default "commentator"

Return JSON:
{
  ...same event fields...,
  "narrative_title": "...",
  "narrative_summary": "..."
}

- Use narrator.generate_description(event, tone=tone).
- If LLM disabled, fall back to rule-based narrative from narrator.

6) Wire endpoints into FastAPI:
- In backend/app/main.py (or your router setup), include these routes.

7) Add minimal tests (optional but preferred):
- Add a simple test verifying /feed/anomalies returns non-empty items when demo dataset used.
- Keep it light.

After implementing, show:
- backend/app/anomaly_feed.py
- Router registration changes
- Example JSON from /feed/anomalies (first 2 items).
